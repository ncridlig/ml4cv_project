% abstract.tex
\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

% Write 150-250 word abstract

Autonomous Formula SAE race cars must detect colored traffic cones in real time to navigate an unknown track. We improve the cone detection pipeline for the Unibo Motorsport race car by evaluating three YOLO architectures, YOLO11n, YOLO12n, and YOLO26n, on the FSOCO-12 benchmark under identical training conditions. YOLO26n achieves \mAPfifty{}=0.763, a 14.6\% improvement over the YOLO11n model previously deployed on the car (0.666), while being smaller and faster. A Bayesian hyperparameter sweep over 13 parameters found no improvement over the Ultralytics defaults, confirming that architecture selection dominates tuning for this task. A two-stage training strategy, pre-training on a larger 22{,}725-image dataset before fine-tuning on FSOCO-12, matched the single-stage benchmark but improved recall by 1.4 percentage points. To validate beyond internet benchmarks, we created fsoco-ubm, a 96-image test set from the car's own stereo camera, which revealed a 22--27\% accuracy drop across all models. The selected model was deployed via TensorRT FP16 on the onboard RTX~4060. Integrating YOLO26n required modifying the C++ inference node to handle its end-to-end output format, which eliminated NMS postprocessing entirely. The full perception pipeline runs at 15.6~ms, within the 16.7~ms budget for 60~fps operation.
