% chapters/results.tex
\chapter{Results}
\label{ch:results}

We evaluate all models on two test sets: the FSOCO-12 benchmark (689 images) and fsoco-ubm (96 images from the car's own camera), reporting standard COCO metrics \citep{coco}.

\section{FSOCO-12 Benchmark Results}
\label{sec:fsoco_results}

% Discuss overall results

\Cref{tab:fsoco_overall} presents the FSOCO-12 test set results. YOLO26n dominates: the single-stage variant achieves the highest \mAPfifty{}=0.763 and precision (0.849), while the two-stage variant leads in recall (0.708) and \mAPfiftyfive{}=0.528. Both YOLO26n variants outperform the UBM production model by over 14\%, confirming that the architecture upgrade accounts for the bulk of the improvement. YOLO12n and our retrained YOLO11n baseline land nearly identically at \mAPfifty{}$\approx$0.708, both already 6\% above the production model. 
\begin{table}[ht]
\centering
\caption{Overall performance on the FSOCO-12 test set (689 images, 12{,}054 instances).}
\label{tab:fsoco_overall}
\begin{tabular}{@{}llS[table-format=3.0]S[table-format=1.3]S[table-format=1.3]S[table-format=1.3]S[table-format=1.3]@{}}
\toprule
\textbf{Model} & \textbf{Training Data} & {\textbf{Epochs}} & {\textbf{\mAPfifty{}}} & {\textbf{\mAPfiftyfive{}}} & {\textbf{Prec.}} & {\textbf{Recall}} \\ \midrule
YOLO26n (single)    & FSOCO-12       & 300     & \textbf{0.763} & 0.524          & \textbf{0.849} & 0.694          \\
YOLO26n (two-stage) & CD$\to$FSOCO-12 & {338+300} & 0.761          & \textbf{0.528} & 0.832          & \textbf{0.708} \\
YOLO12n             & FSOCO-12       & 300     & 0.708          & 0.485          & 0.840          & 0.654          \\
YOLO11n (ours)     & FSOCO-12       & 300     & 0.707          & 0.490          & 0.816          & 0.662          \\
UBM Production      & unknown        & 300     & 0.666          & 0.461          & 0.803          & 0.579          \\ \bottomrule
\end{tabular}
\begin{flushleft}
\small CD = cone-detector dataset (22{,}725 images, pre-training stage).
\end{flushleft}
\end{table}

%% ================================================================
\section{Per-Class Analysis}
\label{sec:perclass}

% Discuss per-class patterns

\Cref{tab:yolo26_perclass} shows a consistent per-class hierarchy. Large orange cones are easiest (\mAPfifty{}=0.886), followed by blue (0.863), yellow (0.856), and orange (0.843). Unknown cones remain the hardest class at \mAPfifty{}=0.364---these are cones that annotators could not confidently classify, so the model inherits that ambiguity. YOLO26n improves every class over YOLO12n, with the largest gain on unknown cones (+23.4\%), likely because it is more willing to commit to a class label on distant cones that occupy as few as $20\times20$ pixels (\cref{fig:gt_detections}).

\begin{figure}[ht]
\centering
\includegraphics[width=0.75\textwidth]{comparison_lidar1_left_gt_detections.jpg}
\caption{YOLO26n (two-stage) detections on a fsoco-ubm image. Distant cones spanning only tens of pixels are detected, though not all.}
\label{fig:gt_detections}
\end{figure}

\begin{table}[ht]
\centering
\caption{Per-class performance of YOLO26n (single-stage) on FSOCO-12 test set.}
\label{tab:yolo26_perclass}
\begin{tabular}{@{}lrrS[table-format=1.3]S[table-format=1.3]S[table-format=1.3]S[table-format=1.3]@{}}
\toprule
\textbf{Class} & \textbf{Images} & \textbf{Instances} & {\textbf{Prec.}} & {\textbf{Recall}} & {\textbf{\mAPfifty{}}} & {\textbf{\mAPfiftyfive{}}} \\ \midrule
Large Orange Cone & 154  & 408    & 0.873 & 0.833 & \textbf{0.886} & 0.688 \\
Blue Cone         & 506  & 4{,}437 & 0.927 & 0.783 & 0.863          & 0.602 \\
Yellow Cone       & 562  & 4{,}844 & 0.915 & 0.774 & 0.856          & 0.583 \\
Orange Cone       & 286  & 1{,}686 & 0.892 & 0.779 & 0.843          & 0.571 \\
Unknown Cone      & 68   & 679    & 0.635 & 0.297 & 0.364          & 0.178 \\ \bottomrule
\end{tabular}
\end{table}


%% ================================================================
\section{Two-Stage vs.\ Single-Stage Training}
\label{sec:two_stage_results}

% Discuss two-stage results

\Cref{tab:two_stage} compares the two training strategies head-to-head. On the FSOCO-12 benchmark the difference is within noise: \mAPfifty{} differs by just 0.2\%, and neither variant is consistently better across all metrics. The two-stage model wins on recall (+1.4 percentage points) and \mAPfiftyfive{} (+0.4 percentage points), at the cost of lower precision ($-1.7$ percentage points). On the real-world fsoco-ubm set, the two models tie at \mAPfifty{}=0.565, but two-stage achieves 3.4 percentage points higher precision, indicating fewer false positives under deployment conditions. We deployed the two-stage model because missing a cone is more dangerous than a false detection in autonomous racing. However, we do not recommend training two stage models due to their quadrupled computational cost; better to train other architectures and improve the datasets.

\begin{table}[ht]
\centering
\caption{Two-stage vs.\ single-stage YOLO26n training comparison.}
\label{tab:two_stage}
\begin{tabular}{@{}lS[table-format=1.3]S[table-format=1.3]r@{}}
\toprule
\textbf{Metric}            & {\textbf{Single-Stage}} & {\textbf{Two-Stage}} & \textbf{Delta}  \\ \midrule
FSOCO-12 \mAPfifty{}       & \textbf{0.763}          & 0.761                & $-0.2$\,pp      \\
FSOCO-12 \mAPfiftyfive{}   & 0.524                   & \textbf{0.528}       & $+0.4$\,pp      \\
FSOCO-12 Precision         & \textbf{0.849}          & 0.832                & $-1.7$\,pp      \\
FSOCO-12 Recall            & 0.694                   & \textbf{0.708}       & $+1.4$\,pp      \\
fsoco-ubm \mAPfifty{}      & 0.565                   & \textbf{0.565}       & $\approx 0$\,pp \\
fsoco-ubm Precision        & 0.615                   & \textbf{0.649}       & $+3.4$\,pp      \\
Generalization Gap         & {$-25.9\%$}             & {$\mathbf{-25.8\%}$} & $+0.1$\,pp      \\ \bottomrule
\end{tabular}
\begin{flushleft}
\small pp = percentage points.
\end{flushleft}
\end{table}

%% ================================================================
\section{Real-World Validation (fsoco-ubm)}
\label{sec:ubm_results}

% Discuss real-world results

\Cref{tab:ubm_results} shows performance on fsoco-ubm. Every model suffers a substantial drop from the FSOCO-12 benchmark, ranging from $-21.5\%$ (YOLO11n) to $-27.0\%$ (YOLO12n). This gap reflects the harder real-world conditions described in \cref{subsec:fsoco_ubm}. Despite the drop, the YOLO26n variants maintain first place at \mAPfifty{}=0.565. An interesting finding is that YOLO11n generalizes best, losing only 21.5\% and achieving the highest precision on this set (0.874), which suggests that its simpler architecture is more conservative and less prone to false positives on out-of-distribution data. YOLO12n generalizes worst among our trained models ($-27.0\%$) and ends up tied with UBM production at 0.517.

\begin{table}[ht]
\centering
\caption{Real-world performance on fsoco-ubm test set (96 images, 1{,}426 instances).}
\label{tab:ubm_results}
\begin{tabular}{@{}lS[table-format=1.3]S[table-format=1.3]S[table-format=1.3]r@{}}
\toprule
\textbf{Model} & {\textbf{\mAPfifty{}}} & {\textbf{Prec.}} & {\textbf{Recall}} & \textbf{Gap vs FSOCO-12} \\ \midrule
YOLO26n (two-stage) & \textbf{0.565} & \textbf{0.649} & 0.462          & $-25.8\%$  \\
YOLO26n (single)    & \textbf{0.565} & 0.615          & \textbf{0.469} & $-25.9\%$  \\
YOLO11n (ours)     & 0.555          & 0.874          & 0.447          & $-21.5\%$  \\
YOLO12n             & 0.517          & 0.572          & 0.454          & $-27.0\%$  \\
UBM Production      & 0.517          & 0.635          & 0.393          & $-22.3\%$  \\ \bottomrule
\end{tabular}
\end{table}

%% ================================================================
\section{Deployment Performance}
\label{sec:deployment_results}

% Discuss deployment results

On the RTX 4060 onboard the race car, the YOLO26n TensorRT FP16 engine averages 2.63~ms per image in isolated benchmarks (\texttt{trtexec}: 1.58~ms host-to-device, 1.02~ms GPU compute, 0.03~ms device-to-host). The production YOLO11n engine is comparable at 2.70~ms; both models are transfer-bound rather than compute-bound at this scale. Note that model inference is only one stage of the full perception pipeline---preprocessing, stereo matching, feature matching, and triangulation add substantial overhead. As shown in \cref{sec:integration}, the complete pipeline runs at 15.61~ms for YOLO26n, just within the 16.7~ms budget for 60~fps. We did not pursue INT8 quantization: the 1--2\% \mAPfifty{} penalty is not justified when inference itself is not the bottleneck.

%% ================================================================
\section{Integration Challenges}
\label{sec:integration}

Deploying a new YOLO architecture revealed a hidden assumption in the inference pipeline: the output tensor format. When we first loaded the YOLO26n engine, the node produced zero detections with occasional flickering gray bounding boxes (class ``unknown cone''). The TensorRT engine check logs revealed the root cause---a tensor shape mismatch between what the code expected and what the model produced.

\Cref{tab:tensor_formats} shows the critical difference. YOLO11 outputs a tensor of shape $(2, 9, 8400)$, where the 9 channels encode four bounding box coordinates (center $x$, center $y$, width, height) plus five class confidences, and 8400 is the total number of anchor-free predictions across three feature map scales ($80^2 + 40^2 + 20^2 = 8{,}400$ for a $640\times640$ input). These raw predictions require Non-Maximum Suppression (NMS) in postprocessing to filter overlapping detections. YOLO26, following the end-to-end paradigm introduced by YOLOv10 \citep{yolov10}, performs NMS internally via consistent dual assignments during training and outputs $(2, 300, 6)$: up to 300 pre-filtered detections per image \citep{yolo26, yolo26_docs}. Each detection contains six values: corner coordinates ($x_1, y_1, x_2, y_2$), confidence, and class ID. The old postprocessing code assumed the YOLO11 layout, reading memory at wrong strides and interpreting random floats as coordinates and class IDs, occasionally producing a garbage detection that passed validation.

\begin{table}[ht]
\centering
\caption{Output tensor formats for legacy and end-to-end YOLO architectures.}
\label{tab:tensor_formats}
\begin{tabular}{@{}lllll@{}}
\toprule
\textbf{Model} & \textbf{Output Shape} & \textbf{Format} & \textbf{Box Encoding} & \textbf{NMS} \\ \midrule
YOLO11 & $(2, 9, 8400)$ & $[\text{batch}, 4{+}n_c, \text{anchors}]$ & $x_c, y_c, w, h$ & Software \\
YOLO26 & $(2, 300, 6)$ & $[\text{batch}, \text{max\_det}, 6]$ & $x_1, y_1, x_2, y_2$ & Built-in \\ \bottomrule
\end{tabular}
\begin{flushleft}
\small $n_c$ = number of classes (5). Anchors: $80^2{+}40^2{+}20^2 = 8{,}400$ for $640{\times}640$ input.
\end{flushleft}
\end{table}

The fix auto-detects the model version from output dimensions at initialization: if \texttt{d[2]==6}, the model is end-to-end; if \texttt{d[1]<d[2]}, it is legacy. Each format has its own postprocessing path. The change required approximately 40 lines of new code with zero modifications to the existing YOLO11 path, which was simply wrapped in a conditional block. This design maintains backward compatibility: the same node binary can load either model type without recompilation.

\Cref{tab:timing_comparison} compares per-stage timing between the two architectures on the full inference pipeline. The most striking result is postprocessing: YOLO11 spends 0.35~ms on NMS, while YOLO26 registers 0.00~ms (below measurement resolution) because NMS is eliminated entirely. The total pipeline time drops from 16.56~ms to 15.61~ms, a 5.7\% improvement. Feature matching is also 18\% faster with YOLO26, likely because the end-to-end model produces fewer false positives and thus fewer bounding boxes to match across the stereo pair.

\begin{table}[ht]
\centering
\caption{Per-stage timing comparison on RTX 4060 (mean over $>$10{,}000 frames).}
\label{tab:timing_comparison}
\begin{tabular}{@{}lS[table-format=1.2]S[table-format=1.2]r@{}}
\toprule
\textbf{Stage} & {\textbf{YOLO11 (ms)}} & {\textbf{YOLO26 (ms)}} & \textbf{Delta} \\ \midrule
Preprocessing      & 0.32 & 0.31 & $-3\%$ \\
Inference          & 5.42 & 5.29 & $-2\%$ \\
\textbf{Postprocessing} & \textbf{0.35} & \textbf{0.00} & $\mathbf{-100\%}$ \\
BBox matching      & 2.35 & 2.40 & $+2\%$ \\
Feature matching   & 2.88 & 2.37 & $-18\%$ \\
Triangulation      & 0.12 & 0.11 & $-8\%$ \\
Sending            & 0.02 & 0.02 & $0\%$ \\
\textbf{Total}     & \textbf{16.56} & \textbf{15.61} & $\mathbf{-5.7\%}$ \\ \bottomrule
\end{tabular}
\end{table}

Finally, we note that the auto-exposure problem identified in Fusa's thesis \citep{fusa2025} (Section~5.2.2) has been addressed by a fix already present in the codebase. The ZED~2i camera's Auto-Exposure/Auto-Gain Control (AEC/AGC) region is restricted to the bottom half of the frame (y=360 to y=710 in the 720p image), causing the camera to expose for cones on the ground rather than the bright sky. This is a naive but effective solution: since the horizon in track conditions is approximately at the vertical center, the top half (sky) may be over- or underexposed, but cone visibility is preserved. \Cref{fig:yolo26_working} shows YOLO26n correctly detecting cones after all integration issues were resolved.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{yolo26_occlusion.jpg}
\caption{YOLO26n detecting cones in the workshop after integration fixes. The large orange cone and partially occluded yellow cone are both detected with high confidence.}
\label{fig:yolo26_working}
\end{figure}

