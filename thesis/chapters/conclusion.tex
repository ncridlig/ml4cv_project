% chapters/conclusion.tex
\chapter{Conclusion}
\label{ch:conclusion}

\section{Summary of Contributions}
\label{sec:summary}

% Summarize key results

Of the three architectures evaluated, YOLO26n achieved the best results: \mAPfifty{}=0.763 on FSOCO-12 (+14.6\% over the previous production model) and 2.63~ms model inference on the onboard RTX~4060. The full perception pipeline---including stereo matching, feature matching, and triangulation---runs at 15.61~ms, just within the 16.7~ms budget for 60~fps. A two-stage training variant traded marginal benchmark precision for better recall and real-world generalization. Hyperparameter tuning proved ineffective. Our real-world test set, fsoco-ubm, revealed a 22--27\% accuracy drop from the internet benchmark across all models. This gap would have gone unnoticed without car-specific validation data.

\section{Lessons Learned}
\label{sec:lessons}

% Practical insights

Contributing to the existing codebase was straightforward thanks to the modular software architecture established by \citet{fusa2025}: detection, stereo matching, and triangulation are separate stages, so swapping the YOLO model required only replacing the TensorRT engine and updating the confidence threshold. This report also fills a documentation gap: the previous training configuration was lost when team members graduated, so we have documented every run, dataset version, and export step for reproducibility. The key technical lessons are:

\begin{itemize}
    \item \textbf{Architecture selection matters more than hyperparameter tuning.} The sweep explored 13 parameters across 21 runs and found no improvement over defaults, while upgrading from YOLO11n to YOLO26n gained 7.9\% \mAPfifty{}.
    \item \textbf{Always validate on real data.} FSOCO-12 performance does not reliably predict deployment performance: all models lost 22--27\% on our car's camera data. Without fsoco-ubm, we would have overestimated the deployed model's accuracy.
    \item \textbf{Two-stage training provides marginal gains at this data scale.} Pre-training on 3$\times$ more data improved recall and real-world precision, but the benchmark \mAPfifty{} remained within 0.2\% of single-stage for 4$\times$ the compute, suggesting that either the nano capacity has been saturated or the two dataset distributions overlap too much for pre-training to add value.
    \item \textbf{Verify framework internals.} Two silent failures cost significant debugging time. First, the Ultralytics \texttt{optimizer='auto'} setting overwrites user-specified learning rates with hardcoded defaults, which caused catastrophic forgetting during fine-tuning (\cref{app:optimizer_bug}). Second, the C++ ROS node assumed all YOLO models share the same output tensor layout; when YOLO26n produced a \texttt{(2,\,300,\,6)} tensor instead of the expected \texttt{(2,\,9,\,8400)}, the postprocessing code read memory at wrong strides and produced zero valid detections (\cref{sec:integration}). Neither failure raised an error message; both were only diagnosable by inspecting framework logs rather than user-facing output.
\end{itemize}

\section{Limitations}
\label{sec:limitations}

% Discuss limitations

Several limitations should be noted. We could not reproduce the \mAPfifty{}=0.824 reported in \citet{fusa2025}: the training configuration and dataset version used to produce that figure were lost, and the best we achieved with YOLO11n on FSOCO-12 under default hyperparameters was 0.707, which we adopted as our reproducible baseline. The fsoco-ubm test set is small (96 images) and captures a single track on a single day, so it does not cover the full range of conditions the car will face at competition---different track layouts, rain, and dusk lighting are absent. The unknown cone class remains poorly detected (\mAPfifty{}=0.364), a dataset-level problem rooted in annotator ambiguity. We only evaluated nano-sized models ($\sim$2.5M parameters); larger variants may improve accuracy within the available latency budget. Finally, we did not ablate individual augmentation parameters.

\section{Future Work}
\label{sec:future}

% Propose extensions

We recommend the following directions for continued development:

\begin{itemize}
    \item \textbf{Evaluate larger YOLO26 variants.} Model inference accounts for only 5.29~ms of the 15.61~ms pipeline, so a moderately larger model such as YOLO26s could improve detection of distant and ambiguous cones without exceeding the real-time budget, provided the downstream stages are also optimized.
    \item \textbf{Expand fsoco-ubm.} Add images from different tracks, weather conditions, and times of day. The current 96 images from a single session are a starting point, not a representative benchmark. With 1{,}000+ annotated images, fsoco-ubm could also serve as a fine-tuning set, closing the benchmark-to-deployment gap directly rather than just measuring it.
    \item \textbf{Target the unknown cone class.} Data augmentation strategies such as color jittering and occlusion simulation may help the model learn to flag uncertain detections. Alternatively, merging unknown cones into the nearest color class and combining the two orange cone classes during training could improve overall recall at the cost of classification granularity.
    \item \textbf{Optimize downstream pipeline stages.} Feature matching (2.37~ms) and bounding box matching (2.40~ms) together consume nearly as much time as model inference (5.29~ms), and the full pipeline sits at 15.61~ms with only 1~ms of headroom. Optimizing these stages from CPU to CUDA by computing a dense disparity map then for each left-image detection sampling the median disparity within the bounding box region, eliminating the need to run YOLO or match bounding boxes on the right image,   would both improve real-time margin and create room for a larger detection model.
    \item \textbf{Race-condition testing.} The full autonomous stack ran successfully in a workshop setting (\cref{sec:integration}), but has not yet been tested under race conditions with the new model. The racecar will be tested on track the week of 23-28 of February 2026.
\end{itemize}

UBM will compete at Formula Student Germany from 11-16 August 2026. The goal is to complete all four dynamic events (Trackdrive, Autocross, Skid Pad, and Acceleration) for the first time since last year a broken axle ended the campaign before we could attempt any of them. The priority is reliability over outright speed: finishing every event and collecting data is more valuable than optimizing for a fast lap, as the data enables iteration for the 2027 competition. The improvements in this report contribute to that goal by providing a more accurate, faster, and better documented perception pipeline.
