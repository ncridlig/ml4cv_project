% chapters/conclusion.tex
\chapter{Conclusion}
\label{ch:conclusion}

\section{Summary of Contributions}
\label{sec:summary}

% TODO: Summarize key results (2-3 paragraphs)
% - Systematic evaluation of three YOLO architectures for cone detection
% - YOLO26n achieves 0.763 mAP50 on FSOCO-12 (+14.6% over UBM production)
% - Two-stage training matches single-stage mAP50 but improves recall (+1.4pp)
%   and real-world precision (+3.4pp)
% - Deployment on RTX 4060: 2.63 ms inference (2.58x faster than baseline)
% - Created fsoco-ubm real-world test set (96 images from car camera)
% - Hyperparameter sweep confirmed Ultralytics defaults are near-optimal

We systematically evaluated three generations of YOLO architectures for cone detection in Formula Student Driverless. Training YOLOv11n, YOLO12n, and YOLO26n under identical conditions on the FSOCO-12 dataset, we found that YOLO26n---the newest and lightest of the three---achieves the best results: \mAPfifty{}=0.763 on the standard benchmark, a 14.6\% improvement over the model previously deployed on the car. A two-stage training strategy, pre-training on the 22{,}725-image cone-detector dataset before fine-tuning on FSOCO-12, yielded comparable benchmark accuracy (0.761 vs 0.763) but better recall (+1.4 percentage points) and higher real-world precision (+3.4 percentage points). A Bayesian hyperparameter sweep over 13 parameters demonstrated that the Ultralytics defaults are already near-optimal, and that further gains from tuning alone are unlikely.

To validate our results beyond internet benchmarks, we created fsoco-ubm, a 96-image test set from the car's own ZED~2i stereo camera at the Rioveggio test track. All models lost 22--27\% accuracy on this set compared to FSOCO-12, confirming that curated benchmarks overstate deployment performance. Despite this drop, YOLO26n maintained first place. We deployed the final model via TensorRT FP16 on the onboard RTX~4060, achieving 2.63~ms inference latency---a 6.3$\times$ margin over the 60~fps requirement.

\section{Lessons Learned}
\label{sec:lessons}

% TODO: Practical insights (1-2 paragraphs, bullet list)
% - Architecture matters more than hyperparameter tuning for this task
% - Two-stage training is viable but benefits are marginal on this data scale
% - Real-world performance drops 22-32% from benchmark — always validate on real data
% - Ultralytics optimizer='auto' silently overrides lr0 — document pitfalls
% - TensorRT FP16 is sufficient; INT8 unnecessary given 6.3x margin

Contributing to the existing codebase was straightforward thanks to the modular software architecture established by \citet{fusa2025}: detection, stereo matching, and triangulation are separate stages, so swapping the YOLO model required only replacing the TensorRT engine and updating the confidence threshold. This report also fills a documentation gap. The previous training configuration, hyperparameters, and dataset version were lost when team members graduated, making it impossible to reproduce the originally reported results. We have documented every training run, dataset version, and export step so that future team members can reproduce and extend this work. The key technical lessons are:

\begin{itemize}
    \item \textbf{Architecture selection matters more than hyperparameter tuning.} The sweep explored 13 parameters across 21 runs and found no improvement over defaults, while upgrading from YOLOv11n to YOLO26n gained 7.9\% \mAPfifty{}.
    \item \textbf{Always validate on real data.} FSOCO-12 performance does not reliably predict deployment performance: all models lost 22--27\% on our car's camera data. Without fsoco-ubm, we would have overestimated the deployed model's accuracy.
    \item \textbf{Two-stage training provides marginal gains at this data scale.} Pre-training on 3$\times$ more data improved recall and real-world precision, but the benchmark \mAPfifty{} remained within 0.2\% of single-stage. The 43 hours of additional compute are hard to justify for the modest improvement.
    \item \textbf{Verify framework internals.} The Ultralytics \texttt{optimizer='auto'} setting silently overwrites user-specified learning rates, which caused catastrophic forgetting during fine-tuning and cost a full day of debugging (\cref{app:optimizer_bug}).
\end{itemize}

\section{Limitations}
\label{sec:limitations}

% TODO: Discuss limitations (1 paragraph)
% - fsoco-ubm test set small (96 images, single track, single day)
% - Could not reproduce thesis mAP50 = 0.824 claim
% - Unknown Cone class remains challenging (mAP50 = 0.364)
% - No ablation study on individual augmentation parameters
% - Only nano variants tested (no small/medium/large comparison)

Several limitations should be noted. We could not reproduce the \mAPfifty{}=0.824 reported in \citet{fusa2025}: the training configuration and dataset version used to produce that figure were lost, and the best we achieved with YOLOv11n on FSOCO-12 under default hyperparameters was 0.707, which we adopted as our reproducible baseline. The fsoco-ubm test set is small (96 images) and captures a single track on a single day, so it does not cover the full range of conditions the car will face at competition---different track layouts, rain, and dusk lighting are absent. The unknown cone class remains poorly detected at \mAPfifty{}=0.364, which is a dataset-level problem: the class is defined by annotator uncertainty rather than a consistent visual feature, so the model inherits that ambiguity. We only evaluated nano-sized models ($\sim$2.5M parameters); larger variants may achieve higher accuracy within the available latency budget but were not tested. Finally, we did not run ablation studies on individual augmentation parameters, so we cannot isolate which strategies (mosaic, mixup, copy-paste) are most effective for cone detection.

\section{Future Work}
\label{sec:future}

% TODO: Propose extensions (bullet list)
% - Expand fsoco-ubm with more tracks, weather conditions, lighting
% - Test YOLO26s/m for potential accuracy gains (if latency budget allows)
% - Targeted augmentation for Unknown Cone class
% - Auto-exposure compensation in preprocessing pipeline
% - End-to-end ROS2 integration testing with full autonomous stack
% - Cross-camera validation (different stereo cameras)

We recommend the following directions for continued development:

\begin{itemize}
    \item \textbf{Evaluate larger YOLO26 variants.} The 6.3$\times$ latency margin leaves room for the small or medium variants, which may improve detection of distant and ambiguous cones. Even a model 3$\times$ slower would remain real-time.
    \item \textbf{Expand fsoco-ubm.} Add images from different tracks, weather conditions, and times of day. The current 96 images from a single session are a starting point, not a representative benchmark.
    \item \textbf{Target the unknown cone class.} Data augmentation strategies such as color jittering and occlusion simulation may help the model learn to flag uncertain detections. Alternatively, merging unknown cones into the nearest color class during training could improve overall recall at the cost of classification granularity.
    \item \textbf{Auto-exposure compensation.} The ZED~2i camera's auto-exposure underexposes cones when the sky dominates the frame. A preprocessing step that adjusts exposure based on the lower half of the image could reduce this failure mode.
    \item \textbf{End-to-end integration testing.} We validated detection accuracy and inference speed independently but did not test the full autonomous stack (detection $\to$ matching $\to$ triangulation $\to$ SLAM $\to$ planning $\to$ control) with the new model under race conditions.
\end{itemize}

UBM will compete at Formula Student Germany from 11--16 August 2026. The goal is to complete all four dynamic events---Trackdrive, Autocross, Skid Pad, and Acceleration---for the first time; last year a broken axle ended the campaign before we could attempt all of them. The priority is reliability over outright speed: finishing every event and collecting data is more valuable than optimizing for a single fast lap, because the data enables iteration. The improvements in this report contribute to that goal by providing a more accurate and faster perception pipeline, but the real test will come at competition.
