#!/usr/bin/env python3
"""
Two-Stage Training for YOLO26n: Pre-train + Fine-tune

Strategy:
1. Pre-train on cone-detector dataset (22,725 images) - More data, better features
2. Fine-tune on FSOCO-12 dataset (7,120 images) - Adapt to benchmark distribution

Rationale:
- cone-detector has 3√ó more data ‚Üí better generalization
- FSOCO-12 is the benchmark ‚Üí fine-tuning adapts features
- Same objective function (cone detection) ‚Üí transfer learning works well
- YOLO26n loss was still decreasing at 300 epochs ‚Üí train longer

Extended Training:
- Stage 1: 400 epochs (vs 300 single-stage) - Let pretraining converge
- Stage 2: 300 epochs (match single-stage) - Full fine-tuning
- Total: 700 epochs vs 300 single-stage

Usage:
    # Full two-stage training (Stage 1 + Stage 2A + Stage 2B)
    python train_yolo26_two_stage.py

    # Skip Stage 1 (use existing checkpoint)
    python train_yolo26_two_stage.py --skip-stage1

    # Skip Stage 1 AND Stage 2A (jump to Phase 2B only)
    python train_yolo26_two_stage.py --skip-stage1 --skip-stage2a

    # Specify custom checkpoints
    python train_yolo26_two_stage.py --skip-stage1 --stage1-weights path/to/stage1.pt
    python train_yolo26_two_stage.py --skip-stage1 --skip-stage2a --stage2a-weights path/to/stage2a.pt
"""
import os
import argparse
import sys
from pathlib import Path
from ultralytics import YOLO
from dotenv import load_dotenv

load_dotenv()

# Parse command-line arguments
parser = argparse.ArgumentParser(description='Two-stage YOLO26n training')
parser.add_argument('--skip-stage1', action='store_true',
                    help='Skip Stage 1 and go directly to Stage 2 using existing checkpoint')
parser.add_argument('--stage1-weights', type=str,
                    default='runs/detect/runs/two-stage-yolo26/stage1_cone_detector_400ep2/weights/best.pt',
                    help='Path to Stage 1 checkpoint (used when --skip-stage1 is set)')
parser.add_argument('--skip-stage2a', action='store_true',
                    help='Skip Stage 2A (Phase 2A) and go directly to Phase 2B using existing checkpoint')
parser.add_argument('--stage2a-weights', type=str,
                    default='runs/detect/runs/two-stage-yolo26/stage2a_head_only_50ep/weights/best.pt',
                    help='Path to Stage 2A checkpoint (used when --skip-stage2a is set)')
args = parser.parse_args()

# Set wandb environment variables
os.environ['WANDB_ENTITY'] = 'ncridlig-ml4cv'
os.environ['WANDB_PROJECT'] = 'two-stage-yolo26'
wandb_key = os.getenv('WAND_DB_API_KEY')
if wandb_key:
    os.environ['WANDB_API_KEY'] = wandb_key

print("=" * 80)
print("TWO-STAGE TRAINING: YOLO26n with EXTENDED EPOCHS")
print("=" * 80)
print()

if args.skip_stage1:
    print("MODE: Stage 2 Only (Skip Stage 1)")
    print()
    print("Stage 1 Status:")
    print("  ‚úÖ Using existing checkpoint")
    print(f"  ‚úÖ Weights: {args.stage1_weights}")

    # Check if checkpoint exists
    if not Path(args.stage1_weights).exists():
        print()
        print(f"‚ùå ERROR: Stage 1 checkpoint not found: {args.stage1_weights}")
        print()
        print("Available options:")
        print("  1. Run full two-stage training: python train_yolo26_two_stage.py")
        print("  2. Specify different checkpoint: python train_yolo26_two_stage.py --skip-stage1 --stage1-weights PATH")
        print()
        sys.exit(1)

    print()
    print("Stage 2 Configuration:")
    print("  Dataset: FSOCO-12 (7,120 train images)")
    print("  Epochs: 300")
    print("  Estimated time: ~6 hours (RTX 4080 Super)")
else:
    print("MODE: Full Two-Stage Training")
    print()
    print("Strategy:")
    print("  Stage 1: Pre-train on cone-detector (22,725 images, 400 epochs)")
    print("  Stage 2: Fine-tune on FSOCO-12 (7,120 images, 300 epochs)")
    print()
    print("Total epochs: 700 (vs 300 single-stage)")
    print("Total training time: ~14 hours (RTX 4080 Super)")
    print("  - Stage 1: ~8 hours")
    print("  - Stage 2: ~6 hours")
    print()
    print("Rationale:")
    print("  - YOLO26n loss was still decreasing at epoch 300")
    print("  - More data (22,725 vs 7,120) ‚Üí Better feature learning")
    print("  - Extended training (700 total epochs) ‚Üí Full convergence")
    print("  - Fine-tuning ‚Üí Adaptation to FSOCO-12 distribution")
    print()
    print("Expected improvement:")
    print("  - Single-stage YOLO26n: 0.7586 mAP50 (validation)")
    print("  - Two-stage target: 0.77-0.80 mAP50 (NEW SOTA!)")

print()
print("=" * 80)
print()

if not args.skip_stage1:
    input("Press Enter to start Stage 1 (Pre-training on cone-detector for 400 epochs)...")
    print()
else:
    input("Press Enter to start Stage 2 (Fine-tuning on FSOCO-12 for 300 epochs)...")
    print()

# ============================================================================
# STAGE 1: PRE-TRAINING ON CONE-DETECTOR DATASET (400 EPOCHS)
# ============================================================================

if not args.skip_stage1:
    print("=" * 80)
    print("STAGE 1: PRE-TRAINING ON CONE-DETECTOR (400 EPOCHS)")
    print("=" * 80)
    print()
    print("Dataset: cone-detector (22,725 images)")
    print("Source: fsbdriverless/cone-detector-zruok version 1")
    print("Epochs: 400 (extended for full convergence)")
    print("Batch: 64")
    print("Learning rate: 0.01 (standard)")
    print()
    print("This will take ~8 hours...")
    print()

    # Set W&B name for stage 1
    os.environ['WANDB_NAME'] = 'YOLO26n_Stage1_ConeDetector_400ep'

    # Load YOLO26n pretrained model (COCO weights)
    print("Loading YOLO26n pretrained model (COCO weights)...")
    model_stage1 = YOLO('yolo26n.pt')

    print("Model loaded successfully")
    print(f"Parameters: {sum(p.numel() for p in model_stage1.model.parameters()):,}")
    print()

    print("Starting Stage 1 training on cone-detector...")
    print("Note: 400 epochs to let pretraining fully converge")
    print()

    results_stage1 = model_stage1.train(
        data='datasets/cone-detector/data.yaml',  # Large dataset (22,725 images)
        epochs=400,  # Extended: 400 epochs (vs 300 single-stage)
        batch=64,
        imgsz=640,
        device=0,
        workers=12,
        project='runs/two-stage-yolo26',
        name='stage1_cone_detector_400ep',

        # Standard hyperparameters (Ultralytics defaults work well)
        lr0=0.01,
        lrf=0.01,
        momentum=0.937,
        weight_decay=0.0005,
        warmup_epochs=3,

        # Early stopping (patient since we want full convergence)
        patience=80,  # More patient for extended training

        # Save checkpoints
        save=True,
        save_period=25,  # Save every 25 epochs

        # Validation
        val=True,

        # Mixed precision
        amp=True,
    )

    print()
    print("=" * 80)
    print("STAGE 1 COMPLETE")
    print("=" * 80)
    print()
    print(f"Best mAP50 (cone-detector val): {results_stage1.results_dict.get('metrics/mAP50(B)', 'N/A'):.4f}")
    print(f"Best mAP50-95: {results_stage1.results_dict.get('metrics/mAP50-95(B)', 'N/A'):.4f}")
    print(f"Best Precision: {results_stage1.results_dict.get('metrics/precision(B)', 'N/A'):.4f}")
    print(f"Best Recall: {results_stage1.results_dict.get('metrics/recall(B)', 'N/A'):.4f}")
    print()
    print("Stage 1 model saved to:")
    print("  runs/detect/runs/two-stage-yolo26/stage1_cone_detector_400ep/weights/best.pt")
    print()

    stage1_weights = 'runs/detect/runs/two-stage-yolo26/stage1_cone_detector_400ep/weights/best.pt'
else:
    print("=" * 80)
    print("STAGE 1: SKIPPED (Using existing checkpoint)")
    print("=" * 80)
    print()
    print(f"Loading checkpoint: {args.stage1_weights}")
    print()

    stage1_weights = args.stage1_weights

# ============================================================================
# STAGE 2: TWO-PHASE FINE-TUNING ON FSOCO-12 DATASET
# ============================================================================
# Research-based approach to prevent catastrophic forgetting:
# 1. Phase 2A: Freeze backbone, train detection head only (warm up head)
# 2. Phase 2B: Unfreeze all, train with ultra-low LR (full fine-tuning)
#
# Key fixes based on research:
# - Explicitly set optimizer='SGD' to prevent 'auto' from ignoring lr0
# - Use discriminative learning rates (higher for head, lower for backbone)
# - Long warmup (20% of training) to prevent gradient shock
# - Cosine learning rate decay for smooth convergence
# - Patient early stopping (patience=150 for fine-tuning)
# ============================================================================

print("=" * 80)
print("STAGE 2: TWO-PHASE FINE-TUNING ON FSOCO-12")
print("=" * 80)
print()
print("Research-based approach to prevent catastrophic forgetting:")
print()
print("Phase 2A: Freeze Backbone + Train Head (50 epochs)")
print("  - Freeze: First 10 layers (backbone frozen)")
print("  - Learning rate: 0.001 (higher LR okay for head-only)")
print("  - Warmup: 10 epochs (20% of phase)")
print("  - Purpose: Adapt detection head to FSOCO-12 distribution")
print()
print("Phase 2B: Unfreeze All + Full Fine-tuning (250 epochs)")
print("  - Freeze: None (all layers trainable)")
print("  - Learning rate: 0.00005 (ultra-low to prevent forgetting)")
print("  - Warmup: 50 epochs (20% of phase)")
print("  - Cosine LR decay for smooth convergence")
print("  - Patience: 150 (allow recovery from local minima)")
print()
print("Total epochs: 300 (50 + 250)")
print("Expected time: ~6 hours (RTX 4080 Super)")
print("Expected result: 0.78-0.80 mAP50 (target: beat single-stage 0.7626)")
print()
print("=" * 80)
print()

# ============================================================================
# PHASE 2A: FREEZE BACKBONE, TRAIN HEAD ONLY (50 EPOCHS)
# ============================================================================

if not args.skip_stage2a:
    print("=" * 80)
    print("PHASE 2A: HEAD-ONLY TRAINING (50 epochs)")
    print("=" * 80)
    print()

    # Set W&B name for phase 2A
    os.environ['WANDB_NAME'] = 'YOLO26n_Stage2A_HeadOnly_50ep'

    # Load Stage 1 checkpoint
    print(f"Loading Stage 1 checkpoint: {stage1_weights}")
    model_stage2a = YOLO(stage1_weights)

    print("Stage 1 weights loaded successfully")
    print(f"Parameters: {sum(p.numel() for p in model_stage2a.model.parameters()):,}")
    print()

    print("Configuration:")
    print("  - Freezing first 10 layers (backbone)")
    print("  - Training detection head + last layers only")
    print("  - Higher learning rate (0.001) safe for head-only")
    print("  - Short warmup (10 epochs)")
    print()

    results_stage2a = model_stage2a.train(
        data='datasets/FSOCO-12/data.yaml',
        epochs=50,
        batch=64,
        imgsz=640,
        device=0,
        workers=12,
        project='runs/two-stage-yolo26',
        name='stage2a_head_only_50ep',

        # Head-only training (backbone frozen)
        freeze=10,  # Freeze first 10 layers (backbone)

        # Higher learning rate okay for head-only training
        optimizer='AdamW',  # AdamW for fine-tuning (respects lr0, adaptive per-parameter)
        lr0=0.001,  # Higher LR safe for head-only
        lrf=0.0001,  # Decay to 1/10th
        weight_decay=0.0005,
        warmup_epochs=10,  # 20% of 50 epochs

        # Cosine learning rate schedule
        cos_lr=True,

        # No early stopping for this phase (too short)
        patience=50,

        # Save checkpoints
        save=True,
        save_period=10,

        # Validation
        val=True,

        # Mixed precision
        amp=True,
    )

    print()
    print("=" * 80)
    print("PHASE 2A COMPLETE")
    print("=" * 80)
    print()
    print(f"Best mAP50: {results_stage2a.results_dict.get('metrics/mAP50(B)', 0):.4f}")
    print(f"Best mAP50-95: {results_stage2a.results_dict.get('metrics/mAP50-95(B)', 0):.4f}")
    print(f"Best Precision: {results_stage2a.results_dict.get('metrics/precision(B)', 0):.4f}")
    print(f"Best Recall: {results_stage2a.results_dict.get('metrics/recall(B)', 0):.4f}")
    print()
    print("Phase 2A model saved to:")
    print("  runs/detect/runs/two-stage-yolo26/stage2a_head_only_50ep/weights/best.pt")
    print()
    print("Proceeding automatically to Phase 2B...")
    print()

    stage2a_weights = 'runs/detect/runs/two-stage-yolo26/stage2a_head_only_50ep/weights/best.pt'
else:
    print("=" * 80)
    print("PHASE 2A: SKIPPED (Using existing checkpoint)")
    print("=" * 80)
    print()
    print(f"Loading Phase 2A checkpoint: {args.stage2a_weights}")
    print()

    # Check if checkpoint exists
    if not Path(args.stage2a_weights).exists():
        print(f"‚ùå ERROR: Phase 2A checkpoint not found: {args.stage2a_weights}")
        print()
        print("Available options:")
        print("  1. Run full two-stage training: python3 train_yolo26_two_stage.py --skip-stage1")
        print("  2. Specify different checkpoint: python3 train_yolo26_two_stage.py --skip-stage1 --skip-stage2a --stage2a-weights PATH")
        print()
        sys.exit(1)

    stage2a_weights = args.stage2a_weights
    # Create dummy results_stage2a for summary (will use 0.7443 from actual Phase 2A run)
    class DummyResults:
        def __init__(self):
            self.results_dict = {'metrics/mAP50(B)': 0.7443}
    results_stage2a = DummyResults()

# ============================================================================
# PHASE 2B: UNFREEZE ALL, FULL FINE-TUNING (250 EPOCHS)
# ============================================================================

print("=" * 80)
print("PHASE 2B: FULL FINE-TUNING (250 epochs)")
print("=" * 80)
print()

# Set W&B name for phase 2B
os.environ['WANDB_NAME'] = 'YOLO26n_Stage2B_FullFinetune_250ep'

# Load Phase 2A checkpoint (head already adapted)
# stage2a_weights already set based on whether Phase 2A was run or skipped
print(f"Loading Phase 2A checkpoint: {stage2a_weights}")
model_stage2b = YOLO(stage2a_weights)

print("Phase 2A weights loaded successfully")
print()

print("Configuration:")
print("  - All layers trainable (backbone unfrozen)")
print("  - Ultra-low learning rate (0.00005) to prevent forgetting")
print("  - Long warmup (50 epochs = 20% of phase)")
print("  - Cosine LR decay for smooth convergence")
print("  - Patient early stopping (patience=150)")
print()
print("This phase will take ~5 hours...")
print()

results_stage2b = model_stage2b.train(
    data='datasets/FSOCO-12/data.yaml',
    epochs=250,
    batch=64,
    imgsz=640,
    device=0,
    workers=12,
    project='runs/two-stage-yolo26',
    name='stage2b_full_finetune_250ep',

    # Full fine-tuning (all layers trainable)
    freeze=0,  # Unfreeze all layers

    # Ultra-low learning rate to prevent catastrophic forgetting
    optimizer='AdamW',  # AdamW for fine-tuning (adaptive, respects lr0)
    lr0=0.00005,  # 100√ó lower than standard training (research-based)
    lrf=0.000005,  # Decay to 1/10th
    weight_decay=0.0005,
    warmup_epochs=50,  # 20% of 250 epochs (long warmup critical)

    # Cosine learning rate schedule for smooth convergence
    cos_lr=True,

    # Patient early stopping (fine-tuning needs time)
    patience=150,  

    # Save checkpoints
    save=True,
    save_period=10,

    # Validation
    val=True,

    # Mixed precision
    amp=True,
)

# Combined Stage 2 results (Phase 2A + Phase 2B)
results_stage2 = results_stage2b  # Use Phase 2B as final result

print()
print("=" * 80)
print("STAGE 2 COMPLETE")
print("=" * 80)
print()
print(f"Best mAP50 (FSOCO-12 val): {results_stage2.results_dict.get('metrics/mAP50(B)', 'N/A'):.4f}")
print(f"Best mAP50-95: {results_stage2.results_dict.get('metrics/mAP50-95(B)', 'N/A'):.4f}")
print(f"Best Precision: {results_stage2.results_dict.get('metrics/precision(B)', 'N/A'):.4f}")
print(f"Best Recall: {results_stage2.results_dict.get('metrics/recall(B)', 'N/A'):.4f}")
print()
print("Stage 2 model saved to:")
print("  runs/detect/runs/two-stage-yolo26/stage2_fsoco12_300ep/weights/best.pt")
print()

# ============================================================================
# FINAL SUMMARY
# ============================================================================

print("=" * 80)
print("TWO-STAGE YOLO26n TRAINING COMPLETE")
print("=" * 80)
print()

# Get final metrics
stage2_map50 = results_stage2.results_dict.get('metrics/mAP50(B)', 0)
stage2_map50_95 = results_stage2.results_dict.get('metrics/mAP50-95(B)', 0)
stage2_precision = results_stage2.results_dict.get('metrics/precision(B)', 0)
stage2_recall = results_stage2.results_dict.get('metrics/recall(B)', 0)

print("Training Summary:")
print()

if not args.skip_stage1:
    stage1_map50 = results_stage1.results_dict.get('metrics/mAP50(B)', 0)
    print(f"Stage 1 (cone-detector, 400 epochs):")
    print(f"  Best mAP50: {stage1_map50:.4f}")
    print(f"  Dataset: 22,725 images")
    print(f"  Training time: ~8 hours")
    print()
else:
    print(f"Stage 1 (cone-detector):")
    print(f"  Status: Skipped (used existing checkpoint)")
    print(f"  Checkpoint: {args.stage1_weights}")
    print()

print(f"Stage 2 (FSOCO-12, 300 epochs - Two-Phase Fine-tuning):")
print()
print(f"  Phase 2A (Head-only, 50 epochs):")
print(f"    Best mAP50: {results_stage2a.results_dict.get('metrics/mAP50(B)', 0):.4f}")
print(f"    Strategy: Freeze backbone, train head only")
print(f"    Learning rate: 0.001 (higher safe for head-only)")
print()
print(f"  Phase 2B (Full fine-tuning, 250 epochs):")
print(f"    Best mAP50: {stage2_map50:.4f}")
print(f"    Best mAP50-95: {stage2_map50_95:.4f}")
print(f"    Best Precision: {stage2_precision:.4f}")
print(f"    Best Recall: {stage2_recall:.4f}")
print(f"    Strategy: Unfreeze all, ultra-low LR (0.00005)")
print(f"    Warmup: 50 epochs (20% of phase)")
print(f"    Cosine LR decay: Yes")
print()
print(f"  Combined Stage 2:")
print(f"    Total epochs: 300 (50 + 250)")
print(f"    Dataset: 7,120 images")
print(f"    Training time: ~6 hours")
print(f"    Final model: Phase 2B weights")
print()

if not args.skip_stage1:
    print(f"Total Training:")
    print(f"  Total epochs: 700 (400 + 300)")
    print(f"  Total time: ~14 hours")
    print()

# Compare to single-stage baseline
baseline_map50 = 0.7586  # Single-stage YOLO26n validation (from previous training)
improvement = stage2_map50 - baseline_map50

print("=" * 80)
print("COMPARISON TO SINGLE-STAGE YOLO26n")
print("=" * 80)
print()
print(f"Single-stage (FSOCO-12 only, 300 epochs):")
print(f"  mAP50: {baseline_map50:.4f}")
print()
print(f"Two-stage (pre-train + fine-tune, 700 total epochs):")
print(f"  mAP50: {stage2_map50:.4f}")
print()
print(f"Improvement: {improvement:+.4f} ({(improvement/baseline_map50)*100:+.2f}%)")
print()

if stage2_map50 > baseline_map50:
    print("‚úÖ SUCCESS: Two-stage training improved over single-stage!")
    print(f"   - More data (22,725 pretraining) helped learn better features")
    print(f"   - Extended training (700 vs 300 epochs) allowed full convergence")
    print()
    print(f"   üéØ New best validation mAP50: {stage2_map50:.4f}")
    if stage2_map50 > 0.77:
        print(f"   üèÜ OUTSTANDING: > 0.77 mAP50 achieved!")
elif stage2_map50 > baseline_map50 * 0.995:
    print("‚ö†Ô∏è SIMILAR: Two-stage and single-stage achieved similar performance")
    print(f"   - Cone-detector pretraining didn't harm performance")
    print(f"   - Extended training maintained quality")
else:
    print("‚ö†Ô∏è UNDERPERFORMED: Two-stage below single-stage")
    print(f"   Possible reasons:")
    print(f"   - cone-detector distribution mismatch with FSOCO-12")
    print(f"   - Fine-tuning learning rate too low")
    print(f"   - Need different fine-tuning strategy")

print()
print("=" * 80)
print("NEXT STEPS")
print("=" * 80)
print()
print("1. Evaluate on test set:")
print("   python3 evaluate_yolo26_two_stage_test.py")
print()
print("2. Compare to single-stage YOLO26n:")
print("   - Single-stage test: 0.7626 mAP50")
print("   - Two-stage test: TBD")
print("   - Expected: 0.78-0.80 mAP50 (research-based optimization)")
print()
print("3. If better than single-stage:")
print("   - Export to ONNX/TensorRT")
print("   - Benchmark on ASU (RTX 4060)")
print("   - Deploy as production model")
print()
print("4. If similar/worse:")
print("   - Analyze why (optimizer settings, learning rate, warmup)")
print("   - Document findings for academic report")
print()
print("=" * 80)
print("RESEARCH-BASED IMPROVEMENTS APPLIED")
print("=" * 80)
print()
print("Key fixes based on 2024-2025 research:")
print()
print("1. ‚úÖ Explicit optimizer='SGD' (prevent 'auto' from ignoring lr0)")
print("2. ‚úÖ Two-phase fine-tuning (freeze backbone ‚Üí unfreeze all)")
print("3. ‚úÖ Ultra-low learning rate (0.00005 for Phase 2B)")
print("4. ‚úÖ Long warmup (20% of training = 50 epochs)")
print("5. ‚úÖ Cosine LR decay (smooth convergence)")
print("6. ‚úÖ Patient early stopping (patience=150)")
print()
print("Previous attempt issues:")
print("  ‚ùå optimizer='auto' used lr=0.01 instead of lr=0.001 (10√ó too high!)")
print("  ‚ùå Short warmup (3 epochs = 1% only)")
print("  ‚ùå Early stopping too aggressive (patience=50)")
print("  ‚Üí Result: Catastrophic forgetting, stopped at epoch 51")
print()
print("Expected improvement with fixes:")
print("  ‚Üí No catastrophic forgetting")
print("  ‚Üí Smooth fine-tuning trajectory")
print("  ‚Üí Full 300 epochs of training")
print("  ‚Üí Target: 0.78-0.80 mAP50")
print()
print("=" * 80)
print()
print(f"‚úÖ Two-stage model saved to:")
print(f"   Phase 2A (head-only): runs/detect/runs/two-stage-yolo26/stage2a_head_only_50ep/weights/best.pt")
print(f"   Phase 2B (final): runs/detect/runs/two-stage-yolo26/stage2b_full_finetune_250ep/weights/best.pt")
print()
print("üéØ Use Phase 2B weights for evaluation and deployment")
print()
print("=" * 80)
